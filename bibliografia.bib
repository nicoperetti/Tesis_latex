@Article{VGG,
    author       = "Simonyan, K. and Zisserman, A.",
    title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    journal      = "CoRR",
    volume       = "abs/1409.1556",
    year         = "2014"
}

@misc{mikolov2013efficient,
    title={Efficient Estimation of Word Representations in Vector Space},
    author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
    year={2013},
    eprint={1301.3781},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{chen2015microsoft,
    title={Microsoft COCO Captions: Data Collection and Evaluation Server},
    author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
    year={2015},
    eprint={1504.00325},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@Book{worNet,
    title = {WordNet: An Electronic Lexical Database},
    author = {Christiane Fellbaum},
    year = {1998},
    publisher = {Bradford Books},
}

@InProceedings{PaperDirectors,
author = {Sanchez, Jorge and Luque, Franco and Lichtensztein, Leandro},
title = {A Structured Listwise Approach to Learning to Rank for Image Tagging},
booktitle = {The European Conference on Computer Vision (ECCV) Workshops},
month = {September},
year = {2018}
}

@article{kelley1960gradient,
  title={Gradient theory of optimal flight paths},
  author={Kelley, Henry J},
  journal={Ars Journal},
  volume={30},
  number={10},
  pages={947--954},
  year={1960}
}

@inproceedings{imagenet_cvpr09,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR09},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
        

@article{MikolovSCCD13,
  author    = {Tomas Mikolov and
               Ilya Sutskever and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  journal   = {CoRR},
  volume    = {abs/1310.4546},
  year      = {2013},
  url       = {http://arxiv.org/abs/1310.4546},
  archivePrefix = {arXiv},
  eprint    = {1310.4546},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MikolovSCCD13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VaswaniSPUJGKP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{45610,
title	= {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
author	= {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
year	= {2016},
URL	= {http://arxiv.org/abs/1609.08144},
journal	= {CoRR},
volume	= {abs/1609.08144}
}

@techreport{burges2010from,
author = {Burges, Chris J.C.},
title = {From RankNet to LambdaRank to LambdaMART: An Overview},
year = {2010},
month = {June},
abstract = {LambdaMART is the boosted tree version of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very successful algorithms for solving real world ranking problems: for example an ensemble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and reports, and so here we give a self-contained, detailed and complete description of them.},
url = {https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/},
number = {MSR-TR-2010-82},
}

@InProceedings{taylor2008softrank,
author = {Taylor, Mike and Guiver, John and Robertson, Stephen and Minka, Tom},
title = {SoftRank: Optimising Non-Smooth Rank Metrics},
booktitle = {WSDM 2008},
year = {2008},
month = {February},
abstract = {We address the problem of learning large complex ranking functions. Most IR applications use evaluation metrics that depend only upon the ranks of documents. However, most ranking functions generate document scores, which are sorted to produce a ranking. Hence IR metrics are innately non-smooth with respect to the scores, due to the sort. Unfortunately, many machine learning algorithms require the gradient of a training objective in order to perform the optimization of the model parameters, and because IR metricsare non-smooth, we need to find a smooth proxy objective that can be used for training. We present a new family of training objectives that are derived from the rankdistributions of documents, induced by smoothed scores. We call this approach SoftRank. We focus on a smoothed approximation to Normalized Discounted Cumulative Gain(NDCG), called SoftNDCG and we compare it with three other training objectives in the recent literature. We present two main results.},
url = {https://www.microsoft.com/en-us/research/publication/softrank-optimising-non-smooth-rank-metrics/},
edition = {WSDM 2008},
}

@inproceedings{inproceedings,
author = {Lan, Yanyan and Zhu, Yadong and Guo, Jiafeng and Niu, Shuzi and Cheng, Xueqi},
year = {2014},
month = {09},
pages = {},
title = {Position-Aware ListMLE: A Sequential Learning Process for Ranking},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014}
}

@techreport{cao2007learning,
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
title = {Learning to Rank: From Pairwise Approach to Listwise Approach},
year = {2007},
month = {April},
abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as ‘instances’ in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as ‘instances’ in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top one probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.},
url = {https://www.microsoft.com/en-us/research/publication/learning-to-rank-from-pairwise-approach-to-listwise-approach/},
pages = {9},
number = {MSR-TR-2007-40},
}

@inproceedings{de-marneffe-etal-2014-universal,
    title = "Universal {S}tanford dependencies: A cross-linguistic typology",
    author = "de Marneffe, Marie-Catherine  and
      Dozat, Timothy  and
      Silveira, Natalia  and
      Haverinen, Katri  and
      Ginter, Filip  and
      Nivre, Joakim  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf",
    pages = "4585--4592",
}

@article{DBLP:journals/corr/abs-1812-10546,
  author    = {Daniel A. Galron and
               Yuri M. Brovman and
               Jin Chung and
               Michal Wieja and
               Paul Wang},
  title     = {Deep Item-based Collaborative Filtering for Sparse Implicit Feedback},
  journal   = {CoRR},
  volume    = {abs/1812.10546},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.10546},
  archivePrefix = {arXiv},
  eprint    = {1812.10546},
  timestamp = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-10546},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@ARTICLE{2019arXiv190405985W,
       author = {{Wang}, Chu and {Tang}, Lei and {Bian}, Shujun and {Zhang}, Da and
         {Zhang}, Zuohua and {Wu}, Yongning},
        title = "{Reference Product Search}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Machine Learning},
         year = "2019",
        month = "Apr",
          eid = {arXiv:1904.05985},
        pages = {arXiv:1904.05985},
archivePrefix = {arXiv},
       eprint = {1904.05985},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190405985W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
